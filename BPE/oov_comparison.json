{
  "train_corpus_size": 8,
  "test_corpus_size": 8,
  "word_vocab_size": 150,
  "bpe_vocab_size": 200,
  "test_examples": [
    {
      "text": "Transformers have revolutionized natural language processing by introducing self-attention mechanisms that can capture long-range dependencies in text.",
      "word_tokens": [
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "natural",
        "language",
        "processing",
        "by",
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "that",
        "can",
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "in",
        "<UNK>"
      ],
      "word_oov": [
        true,
        true,
        true,
        false,
        false,
        false,
        false,
        true,
        true,
        true,
        false,
        false,
        true,
        true,
        true,
        false,
        true
      ],
      "word_oov_count": 10,
      "word_oov_percentage": 58.82352941176471,
      "bpe_tokens": [
        "t",
        "ra",
        "n",
        "s",
        "for",
        "m",
        "er",
        "s",
        " ",
        "h",
        "a",
        "ve ",
        " ",
        "re",
        "vo",
        "l",
        "u",
        "tion",
        "i",
        "ze ",
        "d",
        " ",
        "n",
        "at ",
        "ur",
        "al",
        " ",
        "languag",
        "e",
        " ",
        "p",
        "ro",
        "c",
        "es",
        "si",
        "n",
        "g",
        " ",
        "by ",
        " ",
        "in",
        "t",
        "ro",
        "d",
        "u",
        "c",
        "ing",
        " ",
        "se",
        "l",
        "f",
        "-",
        "at ",
        "te",
        "n",
        "tion",
        " ",
        "m",
        "e",
        "ch",
        "an",
        "is ",
        "m",
        "s",
        " ",
        "th",
        "at ",
        " ",
        "ca",
        "n",
        " ",
        "ca",
        "p",
        "t",
        "ur",
        "e",
        " ",
        "l",
        "on",
        "g",
        "-",
        "ra",
        "n",
        "g",
        "e",
        " ",
        "de",
        "p",
        "en",
        "de",
        "n",
        "c",
        "i",
        "es",
        " ",
        "in",
        " ",
        "text ",
        "."
      ],
      "bpe_oov_count": 0,
      "bpe_oov_percentage": 0.0
    },
    {
      "text": "The GPT-4 model demonstrates remarkable capabilities in understanding and generating human-like text across a wide variety of domains and tasks.",
      "word_tokens": [
        "the",
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "in",
        "<UNK>",
        "and",
        "<UNK>",
        "<UNK>",
        "text",
        "<UNK>",
        "a",
        "<UNK>",
        "<UNK>",
        "of",
        "<UNK>",
        "and",
        "<UNK>"
      ],
      "word_oov": [
        false,
        true,
        true,
        true,
        true,
        true,
        false,
        true,
        false,
        true,
        true,
        false,
        true,
        false,
        true,
        true,
        false,
        true,
        false,
        true
      ],
      "word_oov_count": 13,
      "word_oov_percentage": 65.0,
      "bpe_tokens": [
        " the ",
        " ",
        "g",
        "p",
        "t",
        "-",
        "4",
        " ",
        "model",
        " ",
        "de",
        "mo",
        "n",
        "st ",
        "ra",
        "te",
        "s",
        " ",
        "re",
        "m",
        "ar",
        "k",
        "a",
        "b",
        "le",
        " ",
        "ca",
        "p",
        "a",
        "b",
        "i",
        "li",
        "ti",
        "es",
        " ",
        "in",
        " ",
        "un",
        "de",
        "r",
        "st ",
        "and",
        "ing",
        " ",
        "and",
        " ",
        "g",
        "en",
        "er",
        "at ",
        "ing",
        " ",
        "h",
        "u",
        "m",
        "an",
        "-",
        "li",
        "k",
        "e",
        " ",
        "text ",
        " ",
        "ac",
        "ro",
        "s",
        "s",
        " ",
        "a",
        " ",
        "w",
        "i",
        "de",
        " ",
        "v",
        "ar",
        "i",
        "e",
        "t",
        "y",
        " ",
        "of",
        " ",
        "d",
        "o",
        "m",
        "a",
        "in",
        "s",
        " ",
        "and",
        " ",
        "ta",
        "s",
        "k",
        "s."
      ],
      "bpe_oov_count": 0,
      "bpe_oov_percentage": 0.0
    },
    {
      "text": "Tokenization is a critical preprocessing step for all language models. It converts raw text into sequences of tokens that can be processed by neural networks.",
      "word_tokens": [
        "<UNK>",
        "is",
        "a",
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "for",
        "all",
        "language",
        "models.",
        "it",
        "<UNK>",
        "raw",
        "text",
        "into",
        "sequences",
        "of",
        "<UNK>",
        "that",
        "can",
        "<UNK>",
        "<UNK>",
        "by",
        "neural",
        "<UNK>"
      ],
      "word_oov": [
        true,
        false,
        false,
        true,
        true,
        true,
        false,
        false,
        false,
        false,
        false,
        true,
        false,
        false,
        false,
        false,
        false,
        true,
        false,
        false,
        true,
        true,
        false,
        false,
        true
      ],
      "word_oov_count": 9,
      "word_oov_percentage": 36.0,
      "bpe_tokens": [
        "to",
        "ken",
        "i",
        "z",
        "at ",
        "i",
        "on",
        " ",
        "is ",
        " ",
        "a",
        " ",
        "c",
        "ri",
        "tic",
        "al",
        " ",
        "pre",
        "p",
        "ro",
        "c",
        "es",
        "si",
        "n",
        "g",
        " ",
        "st ",
        "e",
        "p",
        " ",
        "for",
        " ",
        "al",
        "l",
        " ",
        "languag",
        "e",
        " ",
        "model",
        "s.",
        " ",
        "it",
        " ",
        "co",
        "n",
        "ve ",
        "r",
        "t",
        "s",
        " ",
        "ra",
        "w",
        " ",
        "text ",
        " ",
        "in",
        "to",
        " ",
        "se",
        "quenc",
        "es",
        " ",
        "of",
        " ",
        "to",
        "ken",
        "s",
        " ",
        "th",
        "at ",
        " ",
        "ca",
        "n",
        " ",
        "be",
        " ",
        "p",
        "ro",
        "c",
        "es",
        "se",
        "d",
        " ",
        "by ",
        " ",
        "n",
        "e",
        "ur",
        "al",
        " ",
        "n",
        "e",
        "t",
        "wor",
        "k",
        "s."
      ],
      "bpe_oov_count": 0,
      "bpe_oov_percentage": 0.0
    },
    {
      "text": "Out-of-vocabulary words present a significant challenge for traditional word-level tokenization approaches. Words like \"untransformable\" or \"hyperparametrization\" might not appear in the training data.",
      "word_tokens": [
        "<UNK>",
        "words",
        "<UNK>",
        "a",
        "<UNK>",
        "<UNK>",
        "for",
        "<UNK>",
        "word-level",
        "<UNK>",
        "<UNK>",
        "words",
        "like",
        "<UNK>",
        "or",
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "in",
        "the",
        "<UNK>",
        "<UNK>"
      ],
      "word_oov": [
        true,
        false,
        true,
        false,
        true,
        true,
        false,
        true,
        false,
        true,
        true,
        false,
        false,
        true,
        false,
        true,
        true,
        true,
        true,
        false,
        false,
        true,
        true
      ],
      "word_oov_count": 14,
      "word_oov_percentage": 60.86956521739131,
      "bpe_tokens": [
        "o",
        "u",
        "t",
        "-",
        "of",
        "-",
        "vocabulary ",
        " ",
        "word",
        "s",
        " ",
        "pres",
        "en",
        "t",
        " ",
        "a",
        " ",
        "si",
        "g",
        "ni",
        "f",
        "ic",
        "an",
        "t",
        " ",
        "ch",
        "al",
        "le",
        "n",
        "g",
        "e",
        " ",
        "for",
        " ",
        "t",
        "ra",
        "d",
        "it",
        "i",
        "on",
        "al",
        " ",
        "word",
        "-",
        "le",
        "ve ",
        "l",
        " ",
        "to",
        "ken",
        "i",
        "z",
        "at ",
        "i",
        "on",
        " ",
        "ap",
        "p",
        "ro",
        "ac",
        "h",
        "es",
        ".",
        " ",
        "word",
        "s",
        " ",
        "li",
        "k",
        "e",
        " ",
        "<UNK>",
        "un",
        "t",
        "ra",
        "n",
        "s",
        "for",
        "m",
        "a",
        "b",
        "le",
        "<UNK>",
        " ",
        "or",
        " ",
        "<UNK>",
        "h",
        "y",
        "p",
        "er",
        "p",
        "ar",
        "a",
        "m",
        "e",
        "t",
        "ri",
        "z",
        "at ",
        "i",
        "on",
        "<UNK>",
        " ",
        "m",
        "i",
        "g",
        "h",
        "t",
        " ",
        "n",
        "o",
        "t",
        " ",
        "ap",
        "p",
        "e",
        "ar",
        " ",
        "in",
        " ",
        " the ",
        " ",
        "t",
        "ra",
        "in",
        "ing",
        " ",
        "d",
        "at ",
        "a",
        "."
      ],
      "bpe_oov_count": 4,
      "bpe_oov_percentage": 3.0303030303030303
    },
    {
      "text": "BPE handles rare words effectively by breaking them into subword units. For example, \"untransformable\" can be broken into \"un\" + \"transform\" + \"able\".",
      "word_tokens": [
        "bpe",
        "<UNK>",
        "rare",
        "words",
        "<UNK>",
        "by",
        "breaking",
        "them",
        "into",
        "subword",
        "<UNK>",
        "for",
        "example,",
        "<UNK>",
        "can",
        "<UNK>",
        "<UNK>",
        "into",
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "<UNK>"
      ],
      "word_oov": [
        false,
        true,
        false,
        false,
        true,
        false,
        false,
        false,
        false,
        false,
        true,
        false,
        false,
        true,
        false,
        true,
        true,
        false,
        true,
        true,
        true,
        true,
        true
      ],
      "word_oov_count": 11,
      "word_oov_percentage": 47.82608695652174,
      "bpe_tokens": [
        "b",
        "p",
        "e",
        " ",
        "h",
        "and",
        "le",
        "s",
        " ",
        "ra",
        "re",
        " ",
        "word",
        "s",
        " ",
        "e",
        "f",
        "f",
        "e",
        "c",
        "ti",
        "ve ",
        "ly ",
        " ",
        "by ",
        " ",
        "b",
        "re",
        "a",
        "k",
        "ing",
        " ",
        " the ",
        "m",
        " ",
        "in",
        "to",
        " ",
        " su",
        "bwor",
        "d",
        " ",
        "un",
        "it",
        "s.",
        " ",
        "for",
        " ",
        "e",
        "x",
        "a",
        "m",
        "p",
        "le",
        ",",
        " ",
        "<UNK>",
        "un",
        "t",
        "ra",
        "n",
        "s",
        "for",
        "m",
        "a",
        "b",
        "le",
        "<UNK>",
        " ",
        "ca",
        "n",
        " ",
        "be",
        " ",
        "b",
        "ro",
        "ken",
        " ",
        "in",
        "to",
        " ",
        "<UNK>",
        "un",
        "<UNK>",
        " ",
        "<UNK>",
        " ",
        "<UNK>",
        "t",
        "ra",
        "n",
        "s",
        "for",
        "m",
        "<UNK>",
        " ",
        "<UNK>",
        " ",
        "<UNK>",
        "a",
        "b",
        "le",
        "<UNK>",
        "."
      ],
      "bpe_oov_count": 10,
      "bpe_oov_percentage": 9.615384615384617
    },
    {
      "text": "Multilingual models benefit greatly from subword tokenization, as it allows them to share vocabulary across languages with similar morphological patterns.",
      "word_tokens": [
        "<UNK>",
        "models",
        "<UNK>",
        "<UNK>",
        "from",
        "subword",
        "<UNK>",
        "as",
        "it",
        "<UNK>",
        "them",
        "to",
        "<UNK>",
        "vocabulary",
        "<UNK>",
        "<UNK>",
        "with",
        "<UNK>",
        "<UNK>",
        "<UNK>"
      ],
      "word_oov": [
        true,
        false,
        true,
        true,
        false,
        false,
        true,
        false,
        false,
        true,
        false,
        false,
        true,
        false,
        true,
        true,
        false,
        true,
        true,
        true
      ],
      "word_oov_count": 11,
      "word_oov_percentage": 55.00000000000001,
      "bpe_tokens": [
        "m",
        "ul",
        "ti",
        "li",
        "n",
        "g",
        "u",
        "al",
        " ",
        "model",
        "s",
        " ",
        "be",
        "n",
        "e",
        "f",
        "it",
        " ",
        "g",
        "re",
        "at ",
        "ly ",
        " ",
        "f",
        "ro",
        "m",
        " ",
        " su",
        "bwor",
        "d",
        " ",
        "to",
        "ken",
        "i",
        "z",
        "at ",
        "i",
        "on",
        ",",
        " ",
        "a s",
        " ",
        "it",
        " ",
        "al",
        "l",
        "ow",
        "s",
        " ",
        " the ",
        "m",
        " ",
        "to",
        " ",
        "s",
        "h",
        "ar",
        "e",
        " ",
        "vocabulary ",
        " ",
        "ac",
        "ro",
        "s",
        "s",
        " ",
        "languag",
        "es",
        " ",
        "w",
        "it",
        "h",
        " ",
        "si",
        "m",
        "i",
        "l",
        "ar",
        " ",
        "mo",
        "r",
        "p",
        "h",
        "ol",
        "o",
        "g",
        "ic",
        "al",
        " ",
        "p",
        "at ",
        "ter",
        "n",
        "s."
      ],
      "bpe_oov_count": 0,
      "bpe_oov_percentage": 0.0
    },
    {
      "text": "The efficiency of tokenization algorithms impacts both training and inference speed of language models. BPE offers a good balance between vocabulary size and sequence length.",
      "word_tokens": [
        "the",
        "efficiency",
        "of",
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "and",
        "<UNK>",
        "<UNK>",
        "of",
        "language",
        "models.",
        "bpe",
        "<UNK>",
        "a",
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "vocabulary",
        "size",
        "and",
        "sequence",
        "<UNK>"
      ],
      "word_oov": [
        false,
        false,
        false,
        true,
        true,
        true,
        true,
        true,
        false,
        true,
        true,
        false,
        false,
        false,
        false,
        true,
        false,
        true,
        true,
        true,
        false,
        false,
        false,
        false,
        true
      ],
      "word_oov_count": 12,
      "word_oov_percentage": 48.0,
      "bpe_tokens": [
        " the ",
        " ",
        "e",
        "f",
        "f",
        "ic",
        "i",
        "en",
        "c",
        "y",
        " ",
        "of",
        " ",
        "to",
        "ken",
        "i",
        "z",
        "at ",
        "i",
        "on",
        " ",
        "al",
        "g",
        "or",
        "it",
        "h",
        "m",
        "s",
        " ",
        "i",
        "m",
        "p",
        "ac",
        "t",
        "s",
        " ",
        "b",
        "o",
        "th",
        " ",
        "t",
        "ra",
        "in",
        "ing",
        " ",
        "and",
        " ",
        "in",
        "f",
        "er",
        "en",
        "c",
        "e",
        " ",
        "s",
        "p",
        "e",
        "e",
        "d",
        " ",
        "of",
        " ",
        "languag",
        "e",
        " ",
        "model",
        "s.",
        " ",
        "b",
        "p",
        "e",
        " ",
        "of",
        "f",
        "er",
        "s",
        " ",
        "a",
        " ",
        "g",
        "o",
        "o",
        "d",
        " ",
        "b",
        "al",
        "an",
        "c",
        "e",
        " ",
        "be",
        "t",
        "w",
        "e",
        "en",
        " ",
        "vocabulary ",
        " ",
        "si",
        "ze ",
        " ",
        "and",
        " ",
        "se",
        "quenc",
        "e",
        " ",
        "le",
        "n",
        "g",
        "th",
        "."
      ],
      "bpe_oov_count": 0,
      "bpe_oov_percentage": 0.0
    },
    {
      "text": "Researchers continue to develop improved tokenization methods, such as Unigram, WordPiece, and SentencePiece, each with their own advantages for specific use cases.",
      "word_tokens": [
        "<UNK>",
        "<UNK>",
        "to",
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "as",
        "<UNK>",
        "<UNK>",
        "and",
        "<UNK>",
        "<UNK>",
        "with",
        "<UNK>",
        "<UNK>",
        "<UNK>",
        "for",
        "<UNK>",
        "use",
        "<UNK>"
      ],
      "word_oov": [
        true,
        true,
        false,
        true,
        true,
        true,
        true,
        true,
        false,
        true,
        true,
        false,
        true,
        true,
        false,
        true,
        true,
        true,
        false,
        true,
        false,
        true
      ],
      "word_oov_count": 16,
      "word_oov_percentage": 72.72727272727273,
      "bpe_tokens": [
        "re",
        "se",
        "ar",
        "ch",
        "er",
        "s",
        " ",
        "co",
        "n",
        "ti",
        "n",
        "u",
        "e",
        " ",
        "to",
        " ",
        "de",
        "ve ",
        "l",
        "op",
        " ",
        "i",
        "m",
        "p",
        "ro",
        "ve ",
        "d",
        " ",
        "to",
        "ken",
        "i",
        "z",
        "at ",
        "i",
        "on",
        " ",
        "m",
        "e",
        "th",
        "o",
        "d",
        "s, ",
        " ",
        " su",
        "ch",
        " ",
        "a s",
        " ",
        "un",
        "i",
        "g",
        "ra",
        "m",
        ",",
        " ",
        "word",
        "p",
        "i",
        "e",
        "c",
        "e",
        ",",
        " ",
        "and",
        " ",
        "se",
        "n",
        "te",
        "n",
        "c",
        "e",
        "p",
        "i",
        "e",
        "c",
        "e",
        ",",
        " ",
        "e",
        "ac",
        "h",
        " ",
        "w",
        "it",
        "h",
        " ",
        " the ",
        "ir",
        " ",
        "ow",
        "n",
        " ",
        "ad",
        "v",
        "an",
        "ta",
        "g",
        "es",
        " ",
        "for",
        " ",
        "s",
        "p",
        "e",
        "c",
        "i",
        "f",
        "ic",
        " ",
        "u",
        "se",
        " ",
        "ca",
        "se",
        "s."
      ],
      "bpe_oov_count": 0,
      "bpe_oov_percentage": 0.0
    }
  ],
  "overall": {
    "total_words": 175,
    "total_word_oov": 96,
    "word_oov_percentage": 54.85714285714286,
    "total_bpe_tokens": 848,
    "total_bpe_oov": 14,
    "bpe_oov_percentage": 1.650943396226415
  }
}