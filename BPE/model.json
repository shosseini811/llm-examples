{
  "vocab_size": 500,
  "vocab": {
    "<PAD>": 0,
    "<UNK>": 1,
    "<BOS>": 2,
    "<EOS>": 3,
    " ": 4,
    "\"": 5,
    "+": 6,
    ",": 7,
    "-": 8,
    ".": 9,
    "4": 10,
    "B": 11,
    "E": 12,
    "F": 13,
    "G": 14,
    "I": 15,
    "M": 16,
    "O": 17,
    "P": 18,
    "R": 19,
    "S": 20,
    "T": 21,
    "U": 22,
    "W": 23,
    "a": 24,
    "b": 25,
    "c": 26,
    "d": 27,
    "e": 28,
    "f": 29,
    "g": 30,
    "h": 31,
    "i": 32,
    "k": 33,
    "l": 34,
    "m": 35,
    "n": 36,
    "o": 37,
    "p": 38,
    "q": 39,
    "r": 40,
    "s": 41,
    "t": 42,
    "u": 43,
    "v": 44,
    "w": 45,
    "x": 46,
    "y": 47,
    "z": 48,
    "s ": 49,
    "an": 50,
    "e ": 51,
    "en": 52,
    "in": 53,
    "or": 54,
    "at": 55,
    " t": 56,
    "on": 57,
    " s": 58,
    "al": 59,
    "ar": 60,
    "ro": 61,
    "er": 62,
    "ion": 63,
    "ing": 64,
    "y ": 65,
    "th": 66,
    "t ": 67,
    "iz": 68,
    "d ": 69,
    "el": 70,
    "ab": 71,
    "s.": 72,
    "for": 73,
    "ce": 74,
    "ken": 75,
    "re": 76,
    "al ": 77,
    "ch": 78,
    "es ": 79,
    "tr": 80,
    "of": 81,
    "oken": 82,
    "izat": 83,
    "ization": 84,
    "wor": 85,
    "ang": 86,
    "ag": 87,
    "pro": 88,
    "ing ": 89,
    " m": 90,
    " c": 91,
    "od": 92,
    "un": 93,
    "okenization": 94,
    "ic": 95,
    "le": 96,
    " \"": 97,
    "ans": 98,
    "ansfor": 99,
    "ansform": 100,
    "vo": 101,
    "lang": 102,
    "langu": 103,
    "languag": 104,
    "int": 105,
    "ap": 106,
    "enc": 107,
    "ex": 108,
    "odel": 109,
    "it": 110,
    "and": 111,
    "ra": 112,
    "ul": 113,
    "word": 114,
    " tokenization": 115,
    ", ": 116,
    "ers ": 117,
    "ed ": 118,
    "ur": 119,
    "language ": 120,
    "proce": 121,
    "proces": 122,
    "process": 123,
    "by ": 124,
    "s th": 125,
    " can": 126,
    "de": 127,
    "he ": 128,
    "il": 129,
    "and ": 130,
    "ac": 131,
    "wi": 132,
    "ie": 133,
    "all": 134,
    "model": 135,
    "into": 136,
    "voc": 137,
    "vocab": 138,
    "vocabul": 139,
    "vocabular": 140,
    "ig": 141,
    "fic": 142,
    "vel": 143,
    "transform": 144,
    "able": 145,
    "able\"": 146,
    "pe": 147,
    "ef": 148,
    " su": 149,
    "im": 150,
    "ut": 151,
    "ural ": 152,
    "ing s": 153,
    "att": 154,
    "ent": 155,
    " me": 156,
    "s that": 157,
    "s that can": 158,
    "e l": 159,
    "ies ": 160,
    "ies in": 161,
    " tex": 162,
    "The ": 163,
    " de": 164,
    "e c": 165,
    "ing and ": 166,
    "li": 167,
    "lik": 168,
    "like ": 169,
    "acro": 170,
    "acros": 171,
    "across ": 172,
    "a ": 173,
    "y of": 174,
    "ain": 175,
    "as": 176,
    "ks.": 177,
    "ical ": 178,
    "ical p": 179,
    "p ": 180,
    " language ": 181,
    " language model": 182,
    " language models.": 183,
    "con": 184,
    "ts ": 185,
    " se": 186,
    " seq": 187,
    " sequ": 188,
    "ence": 189,
    " b": 190,
    " be ": 191,
    "ne": 192,
    "vocabulary ": 193,
    "words ": 194,
    "ific": 195,
    "eng": 196,
    " tra": 197,
    " tokenization ": 198,
    "ach": 199,
    "es.": 200,
    "Wor": 201,
    "Word": 202,
    "untransform": 203,
    "untransformable\"": 204,
    "am": 205,
    "BP": 206,
    "BPE": 207,
    "BPE ": 208,
    "are ": 209,
    " th": 210,
    " the": 211,
    " into": 212,
    " sub": 213,
    "\" ": 214,
    "\" +": 215,
    "\" + \"": 216,
    "it ": 217,
    "as ": 218,
    "ow": 219,
    "with": 220,
    "Pie": 221,
    "Piece": 222,
    "Piece, ": 223
  },
  "merges": {
    "s  ": "s ",
    "a n": "an",
    "e  ": "e ",
    "e n": "en",
    "i n": "in",
    "o r": "or",
    "a t": "at",
    "  t": " t",
    "o n": "on",
    "  s": " s",
    "a l": "al",
    "a r": "ar",
    "r o": "ro",
    "e r": "er",
    "i on": "ion",
    "in g": "ing",
    "y  ": "y ",
    "t h": "th",
    "t  ": "t ",
    "i z": "iz",
    "d  ": "d ",
    "e l": "el",
    "a b": "ab",
    "s .": "s.",
    "f or": "for",
    "c e": "ce",
    "k en": "ken",
    "r e": "re",
    "al  ": "al ",
    "c h": "ch",
    "e s ": "es ",
    "t r": "tr",
    "o f": "of",
    "o ken": "oken",
    "iz at": "izat",
    "izat ion": "ization",
    "w or": "wor",
    "an g": "ang",
    "a g": "ag",
    "p ro": "pro",
    "ing  ": "ing ",
    "  m": " m",
    "  c": " c",
    "o d": "od",
    "u n": "un",
    "oken ization": "okenization",
    "i c": "ic",
    "l e": "le",
    "  \"": " \"",
    "an s": "ans",
    "ans for": "ansfor",
    "ansfor m": "ansform",
    "v o": "vo",
    "l ang": "lang",
    "lang u": "langu",
    "langu ag": "languag",
    "in t": "int",
    "a p": "ap",
    "en c": "enc",
    "e x": "ex",
    "od el": "odel",
    "i t": "it",
    "an d": "and",
    "r a": "ra",
    "u l": "ul",
    "wor d": "word",
    " t okenization": " tokenization",
    ",  ": ", ",
    "er s ": "ers ",
    "e d ": "ed ",
    "u r": "ur",
    "languag e ": "language ",
    "pro ce": "proce",
    "proce s": "proces",
    "proces s": "process",
    "b y ": "by ",
    "s  th": "s th",
    " c an": " can",
    "d e": "de",
    "h e ": "he ",
    "i l": "il",
    "an d ": "and ",
    "a c": "ac",
    "w i": "wi",
    "i e": "ie",
    "al l": "all",
    "m odel": "model",
    "int o": "into",
    "vo c": "voc",
    "voc ab": "vocab",
    "vocab ul": "vocabul",
    "vocabul ar": "vocabular",
    "i g": "ig",
    "f ic": "fic",
    "v el": "vel",
    "tr ansform": "transform",
    "ab le": "able",
    "able \"": "able\"",
    "p e": "pe",
    "e f": "ef",
    " s u": " su",
    "i m": "im",
    "u t": "ut",
    "ur al ": "ural ",
    "ing  s": "ing s",
    "at t": "att",
    "en t": "ent",
    " m e": " me",
    "s th at": "s that",
    "s that  can": "s that can",
    "e  l": "e l",
    "i es ": "ies ",
    "ies  in": "ies in",
    " t ex": " tex",
    "T he ": "The ",
    "  de": " de",
    "e  c": "e c",
    "ing  and ": "ing and ",
    "l i": "li",
    "li k": "lik",
    "lik e ": "like ",
    "ac ro": "acro",
    "acro s": "acros",
    "acros s ": "across ",
    "a  ": "a ",
    "y  of": "y of",
    "a in": "ain",
    "a s": "as",
    "k s.": "ks.",
    "ic al ": "ical ",
    "ical  p": "ical p",
    "p  ": "p ",
    "  language ": " language ",
    " language  model": " language model",
    " language model s.": " language models.",
    "c on": "con",
    "t s ": "ts ",
    " s e": " se",
    " se q": " seq",
    " seq u": " sequ",
    "en ce": "ence",
    "  b": " b",
    " b e ": " be ",
    "n e": "ne",
    "vocabular y ": "vocabulary ",
    "word s ": "words ",
    "i fic": "ific",
    "en g": "eng",
    " t ra": " tra",
    " tokenization  ": " tokenization ",
    "a ch": "ach",
    "e s.": "es.",
    "W or": "Wor",
    "Wor d": "Word",
    "un transform": "untransform",
    "untransform able\"": "untransformable\"",
    "a m": "am",
    "B P": "BP",
    "BP E": "BPE",
    "BPE  ": "BPE ",
    "ar e ": "are ",
    " t h": " th",
    " th e": " the",
    "  into": " into",
    " su b": " sub",
    "\"  ": "\" ",
    "\"  +": "\" +",
    "\" +  \"": "\" + \"",
    "i t ": "it ",
    "a s ": "as ",
    "o w": "ow",
    "wi th": "with",
    "P ie": "Pie",
    "Pie ce": "Piece",
    "Piece , ": "Piece, "
  },
  "special_tokens": {
    "<PAD>": 0,
    "<UNK>": 1,
    "<BOS>": 2,
    "<EOS>": 3
  }
}