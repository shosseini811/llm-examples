{
  "vocab_size": 1000,
  "vocab": {
    "<PAD>": 0,
    "<UNK>": 1,
    "<BOS>": 2,
    "<EOS>": 3,
    " ": 4,
    "!": 5,
    "-": 6,
    ".": 7,
    "B": 8,
    "E": 9,
    "G": 10,
    "H": 11,
    "I": 12,
    "M": 13,
    "P": 14,
    "R": 15,
    "T": 16,
    "a": 17,
    "b": 18,
    "c": 19,
    "d": 20,
    "e": 21,
    "f": 22,
    "g": 23,
    "h": 24,
    "i": 25,
    "j": 26,
    "k": 27,
    "l": 28,
    "m": 29,
    "n": 30,
    "o": 31,
    "p": 32,
    "q": 33,
    "r": 34,
    "s": 35,
    "t": 36,
    "u": 37,
    "v": 38,
    "w": 39,
    "x": 40,
    "y": 41,
    "z": 42,
    "s ": 43,
    " t": 44,
    "e ": 45,
    "en": 46,
    "or": 47,
    "in": 48,
    " to": 49,
    "ken": 50,
    "on": 51,
    "st": 52,
    " token": 53,
    "iz": 54,
    "at": 55,
    "an": 56,
    "is ": 57,
    " tokeniz": 58,
    "ati": 59,
    "ation": 60,
    "er": 61,
    "ing": 62,
    " tokenization": 63,
    "wor": 64,
    "he ": 65,
    "ar": 66,
    "t ": 67,
    "th": 68,
    "d ": 69,
    "es": 70,
    "ch": 71,
    " s": 72,
    "mp": 73,
    "pu": 74,
    " te": 75,
    "ing ": 76,
    "BP": 77,
    "BPE": 78,
    " f": 79,
    "ith": 80,
    "word": 81,
    "and ": 82,
    "ct": 83,
    " the ": 84,
    "mo": 85,
    "qu": 86,
    "of": 87,
    "el": 88,
    "ro": 89,
    "Th": 90,
    "This ": 91,
    "This is ": 92,
    "This is a": 93,
    "imp": 94,
    "for": 95,
    " test": 96,
    "BPE tokenization": 97,
    "BPE tokenization.": 98,
    "s w": 99,
    "s with": 100,
    "s with ": 101,
    "ent ": 102,
    "ur": 103,
    "es.": 104,
    " wor": 105,
    "y ": 106,
    "mer": 107,
    " the mo": 108,
    " the most": 109,
    "ai": 110,
    "air": 111,
    "of ": 112,
    "char": 113,
    "chara": 114,
    "charact": 115,
    "character": 116,
    "s.": 117,
    "The ": 118,
    "al": 119,
    "le": 120,
    " tokenization ": 121,
    "ld": 122,
    "pro": 123,
    "proc": 124,
    "proces": 125,
    "T ": 126,
    "ub": 127,
    "ubword": 128,
    "od": 129
  },
  "merges": {
    "s  ": "s ",
    "  t": " t",
    "e  ": "e ",
    "e n": "en",
    "o r": "or",
    "i n": "in",
    " t o": " to",
    "k en": "ken",
    "o n": "on",
    "s t": "st",
    " to ken": " token",
    "i z": "iz",
    "a t": "at",
    "a n": "an",
    "i s ": "is ",
    " token iz": " tokeniz",
    "at i": "ati",
    "ati on": "ation",
    "e r": "er",
    "in g": "ing",
    " tokeniz ation": " tokenization",
    "w or": "wor",
    "h e ": "he ",
    "a r": "ar",
    "t  ": "t ",
    "t h": "th",
    "d  ": "d ",
    "e s": "es",
    "c h": "ch",
    "  s": " s",
    "m p": "mp",
    "p u": "pu",
    " t e": " te",
    "ing  ": "ing ",
    "B P": "BP",
    "BP E": "BPE",
    "  f": " f",
    "i th": "ith",
    "wor d": "word",
    "an d ": "and ",
    "c t": "ct",
    " t he ": " the ",
    "m o": "mo",
    "q u": "qu",
    "o f": "of",
    "e l": "el",
    "r o": "ro",
    "T h": "Th",
    "Th is ": "This ",
    "This  is ": "This is ",
    "This is  a": "This is a",
    "i mp": "imp",
    "f or": "for",
    " te st": " test",
    "BPE  tokenization": "BPE tokenization",
    "BPE tokenization .": "BPE tokenization.",
    "s  w": "s w",
    "s w ith": "s with",
    "s with  ": "s with ",
    "en t ": "ent ",
    "u r": "ur",
    "es .": "es.",
    "  wor": " wor",
    "y  ": "y ",
    "m er": "mer",
    " the  mo": " the mo",
    " the mo st": " the most",
    "a i": "ai",
    "ai r": "air",
    "of  ": "of ",
    "ch ar": "char",
    "char a": "chara",
    "chara ct": "charact",
    "charact er": "character",
    "s .": "s.",
    "T he ": "The ",
    "a l": "al",
    "l e": "le",
    " tokenization  ": " tokenization ",
    "l d": "ld",
    "p ro": "pro",
    "pro c": "proc",
    "proc es": "proces",
    "T  ": "T ",
    "u b": "ub",
    "ub word": "ubword",
    "o d": "od"
  },
  "special_tokens": {
    "<PAD>": 0,
    "<UNK>": 1,
    "<BOS>": 2,
    "<EOS>": 3
  }
}